# -*- coding: utf-8 -*-
"""Version_Vitgan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ChAZ1bWyWEEnM8bVxtQgJS1bOeGtbHR_
"""

from torch import nn
import torch
from torch.nn.functional import normalize #v=\frac{v}{\max (\|v\|_{p}, \epsilon)}
import torch.nn.functional as F
import math
import torchvision
from torch_ema import ExponentialMovingAverage
from torchvision import transforms
import numpy as np
from tqdm import tqdm
from torch.optim.optimizer import Optimizer
import matplotlib.pyplot as plt 
plt.rcParams["figure.figsize"] = (10,6)

to_pil = transforms.ToPILImage()
renorm = transforms.Normalize((-1.), (2.))

def DiffAugment(x, policy='', channels_first=True):
    if policy:
        if not channels_first:
            x = x.permute(0, 3, 1, 2)
        for p in policy.split(','):
            for f in AUGMENT_FNS[p]:
                x = f(x)
        if not channels_first:
            x = x.permute(0, 2, 3, 1)
        x = x.contiguous()
    return x


def rand_brightness(x):
    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)
    return x


def rand_saturation(x):
    x_mean = x.mean(dim=1, keepdim=True)
    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean
    return x


def rand_contrast(x):
    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)
    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean
    return x


def rand_translation(x, ratio=0.125):
    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)
    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)
    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)
    grid_batch, grid_x, grid_y = torch.meshgrid(
        torch.arange(x.size(0), dtype=torch.long, device=x.device),
        torch.arange(x.size(2), dtype=torch.long, device=x.device),
        torch.arange(x.size(3), dtype=torch.long, device=x.device),
    )
    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)
    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)
    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])
    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()
    return x


def rand_cutout(x, ratio=0.5):
    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)
    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)
    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)
    grid_batch, grid_x, grid_y = torch.meshgrid(
        torch.arange(x.size(0), dtype=torch.long, device=x.device),
        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),
        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),
    )
    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)
    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)
    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)
    mask[grid_batch, grid_x, grid_y] = 0
    x = x * mask.unsqueeze(1)
    return x


AUGMENT_FNS = {
    'color': [rand_brightness, rand_saturation, rand_contrast],
    'translation': [rand_translation],
    'cutout': [rand_cutout],
}

#OK verifier
class PatchEmbedding(nn.Module):
    def __init__(self, image_size = 32, patch_size = 4, in_channels = 3, emb_dim = 384):
        #Patch, Embedding, overlapping
        super(PatchEmbedding,self).__init__()
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) * (image_size // patch_size)
        self.conv = nn.Conv2d(in_channels, emb_dim, kernel_size = 2 * patch_size, stride = patch_size, padding = patch_size//2)
        self.norm = nn.BatchNorm2d(emb_dim)

    def forward(self, x):
        # x: shape BxCxHxW
        #return: B x L x emb_dim with L=H*W/P^2
        _, _, H, W = x.shape
        x = self.conv(x)
        x = self.norm(x)
        x = x.flatten(start_dim=2).transpose(1, 2)
        H, W = H // self.patch_size, W // self.patch_size
        assert H * W == x.shape[1], 'May have the same dimension'
        return x, (H, W)

class SpectralNorm(nn.Module):
    
    def __init__(self, module, name='weight', n_power_iterations=1, eps = 1e-12):
        super(SpectralNorm, self).__init__()
        #------------------------------------------------------------------------------------------
        self.module = module
        self.name = name
        self.n_power_iterations = n_power_iterations
        self.eps = eps
        self.init_spectral = None
        #------------------------------------------------------------------------------------------
        assert n_power_iterations >= 1, "The number of power iterations should be positive integer"
        #------------------------------------------------------------------------------------------ 
        weight = module._parameters[name]
        with torch.no_grad():
            weight_mat = weight.flatten(start_dim = 1)
            out_features, in_features = weight_mat.size()
            u = normalize(weight.new_empty(out_features).normal_(0, 1), dim=0, eps=self.eps)
            v = normalize(weight.new_empty(in_features).normal_(0, 1), dim=0, eps=self.eps)
        #------------------------------------------------------------------------------------------ 
        delattr(module, name)
        module.register_parameter(name + "_orig", weight)
        setattr(module, name, weight.data)
        module.register_buffer(name + "_u", u)
        module.register_buffer(name + "_v", v)
        #------------------------------------------------------------------------------------------ 
        for _ in range(10):
            self.spectral_norm()
        #------------------------------------------------------------------------------------------         
    def spectral_norm(self):
        weight = getattr(self.module, self.name + "_orig")
        u = getattr(self.module, self.name + "_u")
        v = getattr(self.module, self.name + "_v")
        weight_mat = weight.flatten(start_dim = 1)
        #------------------------------------------------------------------------------------------
        with torch.no_grad():
            for _ in range(self.n_power_iterations):
                v = normalize(torch.mv(weight_mat.t(), u), dim=0 ,eps=self.eps, out=v)
                u = normalize(torch.mv(weight_mat, v), dim=0 ,eps=self.eps, out=u)
            #------------------------------------------------------------------------------------------
            u = u.clone(memory_format=torch.contiguous_format)
            v = v.clone(memory_format=torch.contiguous_format)
            #------------------------------------------------------------------------------------------
        sigma = torch.dot(u, torch.mv(weight_mat, v))
        #------------------------------------------------------------------------------------------
        if self.init_spectral is None:
            self.init_spectral = sigma.detach()
        #------------------------------------------------------------------------------------------   
        setattr(self.module, self.name, self.init_spectral * weight / sigma)

    def forward(self, x):
        self.spectral_norm()
        return self.module(x)

class MultiHeadAttention(nn.Module):
    def __init__(self, emb_dim, num_heads=8, discriminator=False, bias_qkv=False, bias_out_att=True, drop_attention=0., drop_out=0.):
        super(MultiHeadAttention, self).__init__()
        self.emb_dim = emb_dim
        self.num_heads = num_heads
        self.head_dim = emb_dim // num_heads
        #self.h_w_size = h_w_size, Tuple of (H,W)
        self.scale = self.head_dim ** -0.5
        self.discriminator = discriminator
        assert self.head_dim * self.num_heads == self.emb_dim, "emb_dim must be divisible by num_heads"
        
        #Weight Tie if discriminator
        #Q = XW_{q} , K = XW_{k} , and V = XW_{v}
        self.drop_attention = nn.Dropout(drop_attention) 
        self.drop_out = nn.Dropout(drop_out)
        self.out_attention = nn.Linear(emb_dim,emb_dim,bias=bias_out_att)
        
        if discriminator:
            #Spectral Normalization arXiv:1802.05957, As we mentioned above, the spectral norm σ(W) that 
            #we use to regularize each layer of the discriminator is the largest singular value of W
            #2.2 FAST APPROXIMATION OF THE SPECTRAL NORM σ(W) In the article 

            #The Lipschitz Constant of Self-Attention
            #Weight Tying
            #arXiv:2006.04710
            self.proj_qk = SpectralNorm(nn.Conv2d(emb_dim , emb_dim, kernel_size = 3, stride = 1, padding = 1 , bias=bias_qkv))
            self.proj_v  = SpectralNorm(nn.Conv2d(emb_dim , emb_dim, kernel_size = 3, stride = 1, padding = 1 , bias=bias_qkv))
            #self.proj_qk = SpectralNorm(nn.Linear(emb_dim, emb_dim, bias=bias_qkv))
            #self.proj_k = SpectralNorm(nn.Linear(emb_dim, emb_dim, bias=bias_qkv))
            #self.proj_v = SpectralNorm(nn.Linear(emb_dim, emb_dim, bias=bias_qkv))
            self.out_attention = SpectralNorm(self.out_attention)
        
        else:
            self.proj_q = EqualizedLinear(emb_dim, emb_dim, bias=bias_qkv)
            self.proj_k = EqualizedLinear(emb_dim, emb_dim, bias=bias_qkv)
            self.proj_v = EqualizedLinear(emb_dim, emb_dim, bias=bias_qkv)  
            #self.in_proj_qkv = nn.Linear(emb_dim, 3*emb_dim, bias=bias_qkv)
        
    
    def forward(self, x):
        #x: B,N,E + cls_token -> N = L+1
        #return: B,N,E
        
        B, N, E = x.shape
        H = W = int(math.sqrt(N - 1))  #H is equal to W, images H==W
                    
        
        if self.discriminator:
            cls_token, x = torch.split(x, [1, N - 1], dim = 1)
            #Same weight for Q and K
            x = x.transpose(1,2).view(B, E, H, W)
            q_k = self.proj_qk(x).flatten(start_dim=2).transpose(1, 2)#B, N, E
            v   = self.proj_v(x).flatten(start_dim=2).transpose(1, 2)#B, N, E
            
            q_k = torch.cat((cls_token, q_k), dim=1)
            v = torch.cat((cls_token, v), dim=1)
            #qk = self.proj_qk(x)
            #k = self.proj_k(x)
            #v = self.proj_v(x)

            Q = q_k.view(B,N ,self.num_heads,self.head_dim).transpose(1,2)#B,Nhead,N,head_dim
            K = q_k.view(B,N ,self.num_heads,self.head_dim).transpose(1,2)#B,Nhead,N,head_dim
            V = v.view(B,N ,self.num_heads,self.head_dim).transpose(1,2)#B,Nhead,N,head_dim
            attn_weights = (-torch.cdist(Q, K, p=2) * self.scale).softmax(dim=-1) #Min distance, great probability
        
        else:
            #If not discriminator
            #q,k,v = self.in_proj_qkv(x).chunk(3, dim=-1)#B,N,E for All
            q = self.proj_q(x)
            k = self.proj_k(x)
            v = self.proj_v(x)
            
            Q = q.view(B,N,self.num_heads,self.head_dim).transpose(1,2)#B,Nhead,N,head_dim
            K = k.view(B,N,self.num_heads,self.head_dim).transpose(1,2)#B,Nhead,N,head_dim
            V = v.view(B,N,self.num_heads,self.head_dim).transpose(1,2)#B,Nhead,N,head_dim
            attn_weights = (torch.matmul(Q, K.transpose(-1, -2)) * self.scale).softmax(dim=-1)
        
        attn_weights = self.drop_attention(attn_weights)
        attn_out = (torch.matmul(attn_weights,V)).transpose(1, 2).reshape(B, N, E)
        attn_out = self.out_attention(attn_out)
        attn_out = self.drop_out(attn_out)
        
        return attn_out

#The MLP contains two layers with a GELU non-linearity. (arXiv:2010.11929)
class MLP(nn.Module):
    def __init__(self, d_model, hidden_dim=None, out_dim=None, activation=nn.GELU, dropout=0.0, discriminator = False):
        super(MLP, self).__init__()    
        out_dim = out_dim or d_model
        hidden_dim = hidden_dim or d_model
        # why using GeLu see article arXiv:1606.08415 
        if discriminator:
            self.model = nn.Sequential(SpectralNorm(nn.Linear(d_model,hidden_dim)),
                                       activation(),
                                       nn.Dropout(p=dropout),
                                       SpectralNorm(nn.Linear(hidden_dim,out_dim)),
                                       nn.Dropout(p=dropout))
        else:
            self.model = nn.Sequential(EqualizedLinear(d_model,hidden_dim, bias_init=1),
                                       activation(),
                                       nn.Dropout(p=dropout),
                                       EqualizedLinear(hidden_dim,out_dim, bias_init=1),
                                       nn.Dropout(p=dropout))   
    def forward(self, x):
        return self.model(x)

class PatchPositionalEmbedding(nn.Module):
    def __init__(self, emb_dim, sequence_length, cls_token = False):
        #Cls_token for discriminator if equal to true (N+1, E)
        #else Generator (N,E)
        super(PatchPositionalEmbedding, self).__init__()
        self.pos_emb = nn.Linear(1, emb_dim)
        self.sequence_length = sequence_length
        self.cls_token = cls_token
    
    def forward(self):
        if self.cls_token:
            x = torch.linspace(-1, 1, self.sequence_length + 1, requires_grad=False).unsqueeze(1).cuda()
        else:
            x =  torch.linspace(-1, 1, self.sequence_length , requires_grad=False).unsqueeze(1).cuda()
        return torch.sin(self.pos_emb(x))

class CoordinatesPositionalEmbedding(nn.Module):
    def __init__(self, emb_dim, patch_size, dim = 2):
        super(CoordinatesPositionalEmbedding, self).__init__()
        self.pos_emb = nn.Linear(dim, emb_dim)
        self.patch_size = patch_size
        self.dim = dim
    
    def forward(self):
        coordinates = tuple(self.dim * [torch.linspace(-1, 1, self.patch_size, requires_grad=False)])
        mgrid = torch.stack(torch.meshgrid(*coordinates),  dim=-1) #, indexing="xy"
        mgrid = mgrid.reshape(-1, self.dim)
        x = self.pos_emb(mgrid.cuda())
        return torch.sin(x)

class SLN(nn.Module):
    def __init__(self, emb_dim):#Not use Bias Affine transformation
        super().__init__()
        self.layernorm = nn.LayerNorm(emb_dim)
        self.gamma = EqualizedLinear(emb_dim, emb_dim, bias = False)
        self.beta = EqualizedLinear(emb_dim, emb_dim, bias = False)
       
    def forward(self, hidden, w):
        #w=MLP(z)-> [batch_size, emb_dim]
        #z: 2-D tensor with shape [batch_size, latent_dim]
        gamma = self.gamma(w).unsqueeze(1) # if we take batch_size x D we must add 1 to have [batch_size,1,D] then the broadcast is made automaticly
        beta = self.beta(w).unsqueeze(1)
        return gamma * self.layernorm(hidden) + beta

#Progressive Growing of GANs for Improved Quality, Stability, and Variation
#arXiv:1710.10196
class EqualizedLinear(nn.Module):
    def __init__(self, in_features, out_features, bias = True, bias_init = 0, lr_mul = 1.):
        super(EqualizedLinear, self).__init__()
        self.scale = (1 / math.sqrt(in_features)) * lr_mul
        self.weight = nn.Parameter(torch.randn(out_features,in_features).div_(lr_mul))
        self.lr_mul = lr_mul
        if bias:
            self.bias = nn.Parameter(torch.ones(out_features) * bias_init)

        else:
            self.register_parameter('bias', None)

    def forward(self, x): 
      if self.bias is not None:  
        x = F.linear(x, self.weight * self.scale , bias=self.bias * self.lr_mul)
        return x
      return F.linear(x, self.weight * self.scale , bias=self.bias )

class MappingNetwork(nn.Module):
    #Mapping StyleGan2
    def __init__(self, latent_dim, emb_dim, hidden_dim = None, n_layers = 4, lr_mul = 0.01):
        super(MappingNetwork, self).__init__()
        self.latent_dim = latent_dim
        self.emb_dim = emb_dim
        self.hidden_dim = hidden_dim or emb_dim 
        #------------------------------------------------------------------
        layers = []
        inSize =  latent_dim
        for i in range(n_layers - 1):
            layers.append(EqualizedLinear(inSize, self.hidden_dim, lr_mul = lr_mul))
            layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True)) 
            inSize = self.hidden_dim  
        layers.append(EqualizedLinear(inSize, self.emb_dim, lr_mul = lr_mul ))
        layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True)) #When we put it in the affin transformation    
        self.model = nn.Sequential(*layers)
        #------------------------------------------------------------------
    def forward(self, z):
        # Map z to w
        # Normalize z
        # z : Batch , Latent_dim
        # w : Batch , emb_dim
        return self.model(F.normalize(z, dim = -1))

class ModulatedLinear(nn.Module):
    def __init__(self, in_features, out_features, demodulation=True):
        super(ModulatedLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.scale = 1 / math.sqrt(in_features)
        self.weight = nn.Parameter(torch.randn(in_features,out_features))        
        self.demodulation = demodulation


    def forward(self, Efou, y):
        #Efou ->(BxL,PxP,E)
        #y -> (B, L, E)
        #groups in conv1d apply in each num_peatches
        #Weight ->(out_features, in_features)
        #y -> (batch_size, 1, self.in_features)
        batch_size = Efou.shape[0]
        y = y.view(batch_size, self.in_features,1) #(BxL, in_features, 1)
        weight = self.scale * self.weight * y

        if self.demodulation:
            #dconf = torch.rsqrt(weight.pow(2).sum(dim = 1) + 1e-8)
            weight = weight * torch.rsqrt(weight.pow(2).sum(dim = 1) + 1e-8).view(batch_size, 1, self.out_features) # batch, out_features, in_features

        return torch.matmul(Efou, weight)

#Positional Embedding Each positional embedding of ViT networks is a linear projection of patch position followed by a sine activation function. 
#The patch positions are normalized to lie between −1.0 and 1.0.
#arXiv:2006.09661 Super interesent article.
#Activation Sine
class Sine(nn.Module):
    def __init__(self,w_0=30):
        super(Sine, self).__init__()
        self.w_0=w_0

    def forward(self, x):
        # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of factor 30
        # arXiv:2006.09661
        return torch.sin(self.w_0 * x) 

class SineLinear(nn.Module):
    def __init__(self, in_features, out_features, w_0 = 30, c = 6., is_first = False, bias = True, output_linear = False):
        #By default w_0=30
        #c=6 if not the first layer
        #see article arXiv:2006.09661
        #Super Effiecent
        super(SineLinear,self).__init__()
        self.in_features = in_features
        self.out_features = out_features    
        self.is_first = is_first
        self.w_0 = w_0
        self.c = c
        self.linear = nn.Linear(in_features, out_features, bias=bias)
        self.activation = Sine(w_0 = w_0) if not output_linear else None
        self.output_linear = output_linear

        self.weights_init() #Else pytorch initialization (refered in articles) but without sqrt

    def weights_init(self):
        if self.is_first : 
            nn.init.uniform_(self.linear.weight.data,( -1 / self.in_features), (1 / self.in_features))
        else:
            nn.init.uniform_(self.linear.weight.data,-math.sqrt(self.c / self.in_features) / self.w_0, math.sqrt(self.c / self.in_features) / self.w_0)

    def forward(self, x):
        output = self.linear(x)
        if self.output_linear:
            return output
        return self.activation(output)        

class SineModulatedLinear(nn.Module):
    def __init__(self, in_features, out_features, w_0 = 30, c = 6., is_first = False,  output_linear = False, demodulation=True):
        #By default w_0=30
        #c=6 if not the first layer
        #see article arXiv:2006.09661
        #Super Effiecent
        super(SineModulatedLinear,self).__init__()
        self.in_features = in_features
        self.out_features = out_features    
        self.is_first = is_first
        self.w_0 = w_0
        self.c = c
        self.linear = ModulatedLinear(in_features, out_features, demodulation=demodulation)
        self.activation = Sine(w_0 = w_0) if not output_linear else None
        self.output_linear = output_linear

        self.weights_init() #Else pytorch initialization (refered in articles) but without sqrt

    def weights_init(self):
        if self.is_first :
            nn.init.uniform_(self.linear.weight.data,( -1 / self.in_features), (1 / self.in_features))
        else:
            nn.init.uniform_(self.linear.weight.data,-math.sqrt(self.c / self.in_features) / self.w_0, math.sqrt(self.c / self.in_features) / self.w_0)

    def forward(self, x, y):
        output = self.linear(x, y)
        if self.output_linear:
            return output
        return self.activation(output)

#To overcome the detrimental effect of traditional non-linearities like ReLU/tanh on modeling fine details and higher-order derivative of the input signals, 
#Sitzmann et al. proposes to use sinusoidal activation functions that allow explicit supervision on any derivatives of the input signal.
class SIREN(nn.Module):
    def __init__(self, in_features, out_features = 3 , hidden_layer = None, bias=True, n_layers=4, w_0=30., c = 6., output_linear=False):
        super(SIREN, self).__init__()
        self.in_features = in_features
        self.out_features = out_features 
        self.hidden_layer = hidden_layer or in_features
        self.w_0 = w_0

        layers = ([SineLinear(self.in_features, self.hidden_layer, w_0 = self.w_0, c=c ,is_first = True, bias = bias)]
                  + [SineLinear(self.hidden_layer, self.hidden_layer, w_0 = self.w_0, c=c,bias = bias) for _ in range(n_layers - 1)]
                  + [SineLinear(self.hidden_layer, self.out_features, w_0 = self.w_0, c=c,bias = bias, output_linear=output_linear)])
        
        self.linear_maps = nn.Sequential(*layers)
        

    def forward(self, x):
        return self.linear_maps(x)

class SIRENModulated(nn.Module):
    def __init__(self, in_features, out_features = 3 , hidden_layer = None, n_layers=4, w_0=30., c = 6., output_linear=False , demodulation = True):
        super(SIRENModulated, self).__init__()
        self.in_features = in_features
        self.out_features = out_features 
        self.hidden_layer = hidden_layer or in_features
        self.w_0 = w_0
        layers = ([SineModulatedLinear(self.in_features, self.hidden_layer, w_0 = self.w_0, c=c ,is_first = True, demodulation = demodulation)]
                  + [SineModulatedLinear(self.hidden_layer, self.hidden_layer, w_0 = self.w_0, c=c, demodulation = demodulation) for _ in range(n_layers - 1)]
                  + [SineModulatedLinear(self.hidden_layer, self.out_features, w_0 = self.w_0, c=c, demodulation = demodulation, output_linear = output_linear)])
        
        self.linear_maps = nn.ModuleList(layers)
        

    def forward(self, x, y):
        for layer in self.linear_maps:
            x = layer(x , y)
        return x


#Fourier Features Let Networks Learn
#High Frequency Functions in Low Dimensional Domains
#arXiv:2006.10739
class FFNEmbedding(nn.Module):
    def __init__(self, in_features = 2, emb_size = 512, scale = 10.):
        #in_features is the dimension (x,y)
        super(FFNEmbedding, self).__init__()
        self.in_features = in_features
        self.emb_size = emb_size 
        self.scale = scale
        self.B = nn.Parameter(torch.randn(in_features, 512//in_features) * scale, requires_grad=False)
        self.out_features = emb_size  # (x,y) so 2 dim
    
    def forward(self, x):
        return torch.cat([torch.sin((2 * math.pi * x) @ self.B),
                          torch.cos((2 * math.pi * x) @ self.B)], dim = -1)

#Use ReLU and Sin instead of Sigmoid (in the original article they use Sigmoid as output), Sin
class FFN(nn.Module):
    def __init__(self, in_features, out_features = 3 , hidden_layer = None, emb_size = 256 ,bias=True, n_layers=4, scale=10., output_linear=False):
        super(FFN, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.hidden_layer = hidden_layer or in_features
        self.scale = scale
        self.output_linear = output_linear 
        self.embedder = FFNEmbedding(in_features, emb_size = emb_size, scale = scale)
        #-----------------------------------------------------------------------------------
        layers = []
        layers.append(nn.Linear(self.embedder.out_features, self.hidden_layer, bias = bias))
        layers.append(nn.ReLU())
        for _ in range(n_layers - 1):
            layers.append(nn.Linear(self.hidden_layer, self.hidden_layer, bias = bias))
            layers.append(nn.ReLU())
        layers.append(nn.Linear(self.hidden_layer, self.out_features, bias = bias))            
        #-----------------------------------------------------------------------------------
        self.linear_maps = nn.Sequential(*layers)

    
    def forward(self, x):
        x = self.embedder(x)
        x = self.linear_maps(x)

        if self.output_linear:
            return x
        return torch.sin(x)

class FFNModulated(nn.Module):
    def __init__(self, in_features, out_features = 3 , hidden_layer = None, emb_size = 256 , n_layers=4, scale=10., output_linear=False, demodulation=True):
        super(FFNModulated, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.hidden_layer = hidden_layer or in_features
        self.scale = scale
        self.output_linear = output_linear 
        self.embedder = FFNEmbedding(in_features, emb_size = emb_size, scale = scale)
        #-----------------------------------------------------------------------------------
        layers = []
        layers.append(ModulatedLinear(self.embedder.out_features, self.hidden_layer, demodulation=demodulation))
        layers.append(nn.ReLU())
        for _ in range(n_layers - 1):
            layers.append(ModulatedLinear(self.hidden_layer, self.hidden_layer, demodulation=demodulation))
            layers.append(nn.ReLU())
        layers.append(ModulatedLinear(self.hidden_layer, self.out_features, demodulation=demodulation))            
        #-----------------------------------------------------------------------------------
        self.linear_maps = nn.ModuleList(layers)

    
    def forward(self, x, y):
        x = self.embedder(x)
        for layer in self.linear_maps:
            x = layer(x, y)
        if self.output_linear:
            return x
        return torch.sin(x)
#Implicit Neural Representation for Patch Generation Each positional embedding is a linear projection of pixel coordinate followed by a sine activation 
# function (hence the name Fourier encoding). The pixel coordinates for P 2 pixels are normalized to lie between −1.0 and 1.0. The 2-layer MLP takes 
# positional embedding Efou as its input, and it is conditioned on patch embedding yi via weight modulation as in (Karras et al., 2020b; Anokhin et al., 2021).

class DisBlock(nn.Module):
    def __init__(self, emb_dim, num_heads=8, mlp_ratio=4., qkv_bias=False, bias_out_att=True, drop_out=0., drop_attention=0., drop_path=0., activation=nn.GELU, norm_layer=nn.LayerNorm):
        super(DisBlock,self).__init__()
        self.norm1 = norm_layer(emb_dim)
        self.attn = MultiHeadAttention(emb_dim, num_heads=num_heads, discriminator=True, bias_qkv=qkv_bias, bias_out_att=bias_out_att, drop_attention=drop_attention, drop_out=drop_attention)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.dropout = nn.Dropout(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(emb_dim)
        mlp_hidden_dim = int(emb_dim * mlp_ratio)
        self.mlp = MLP(emb_dim, hidden_dim=mlp_hidden_dim, activation=activation, dropout=drop_out, discriminator=True)

    def forward(self, x):
        x = x + self.dropout(self.attn(self.norm1(x)))
        x = x + self.dropout(self.mlp(self.norm2(x)))
        return x

class Discriminator(nn.Module):
    def __init__(self, emb_dim=384, image_size=32, patch_size=4, in_channels=3, n_blocks=4,
                 num_heads=6, mlp_ratio=4., qkv_bias=True, bias_out_att=True, drop_out=0., 
                 drop_attention=0., drop_path=0., activation=nn.GELU, norm_layer=nn.LayerNorm):
        super(Discriminator, self).__init__()

        self.emb_dim = emb_dim  
        self.patch_embedding = PatchEmbedding(image_size=image_size, patch_size=patch_size, in_channels=in_channels, emb_dim=emb_dim)
        self.num_patches = self.patch_embedding.num_patches
        self.pos_emb = PatchPositionalEmbedding(emb_dim, self.num_patches, cls_token = True) # (N+1, E)
        self.norm = norm_layer(emb_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))
        
        self.blocks = nn.ModuleList([DisBlock(emb_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, bias_out_att=bias_out_att, drop_out=drop_out, 
                                              drop_attention=drop_attention, drop_path=drop_path, activation=activation, norm_layer=norm_layer)for _ in range(n_blocks)])
        
        #Real or True
        self.final_layer = nn.Linear(self.emb_dim, 1) 

        
    def forward(self, x):
        x , _ = self.patch_embedding(x) # Having patch So Size (B,N,E)
        cls_tokens = self.cls_token.repeat(x.shape[0], 1, 1) 
        x = torch.cat((cls_tokens, x), dim=1) 
        x = x + self.pos_emb() # (B, N+1, E) Knows automaticly to broadcast

        for block in self.blocks:
            x = block(x)

        x = self.norm(x)[:, 0]
        return self.final_layer(x)

class GenBlock(nn.Module):
    def __init__(self, emb_dim, num_heads=8, mlp_ratio=4., qkv_bias=False, bias_out_att=True, drop_out=0., drop_attention=0., drop_path=0., activation=nn.GELU, norm_layer=SLN):
        super(GenBlock,self).__init__()
        self.norm1 = norm_layer(emb_dim)
        self.attn = MultiHeadAttention(emb_dim, num_heads=num_heads, discriminator=False, bias_qkv=qkv_bias, bias_out_att=bias_out_att, drop_attention=drop_attention, drop_out=drop_attention)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.dropout = nn.Dropout(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(emb_dim)
        mlp_hidden_dim = int(emb_dim * mlp_ratio)
        self.mlp = MLP(emb_dim, hidden_dim=mlp_hidden_dim, activation=activation, dropout=drop_out, discriminator=False)

    def forward(self, x, w):
        x = x + self.dropout(self.attn(self.norm1(x, w)))
        x = x + self.dropout(self.mlp(self.norm2(x, w)))
        return x

class Generator(nn.Module):
    def __init__(self, emb_dim=384, latent_dim = 100,image_size=32, patch_size=4, out_features=3, n_blocks=4,
                 num_heads=6, mlp_ratio=4., qkv_bias=True, bias_out_att=True, drop_out=0., demodulation=True,
                 drop_attention=0., drop_path=0., activation=nn.GELU, norm_layer=SLN):
        super(Generator, self).__init__()
        self.num_patches = (image_size//patch_size)**2
        self.image_size  =image_size
        self.patch_size  = patch_size
        self.emb_dim = emb_dim
        self.out_features = out_features
        self.pos_emb = PatchPositionalEmbedding(emb_dim, self.num_patches) # (N, E)
        self.cord_emb = CoordinatesPositionalEmbedding(emb_dim, patch_size)
        self.norm = norm_layer(emb_dim)
        #------------------------------------------------------------------------------------------------------------------------------------------------------------------
        self.mapping_network = MappingNetwork(latent_dim, emb_dim, n_layers=4) 
        self.blocks = nn.ModuleList([GenBlock(emb_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, bias_out_att=bias_out_att, drop_out=drop_out, 
                                              drop_attention=drop_attention, drop_path=drop_path, activation=activation, norm_layer=norm_layer) for _ in range(n_blocks)])
        #------------------------------------------------------------------------------------------------------------------------------------------------------------------
        self.f_theta = SIRENModulated(emb_dim, out_features=out_features, demodulation=demodulation, n_layers=2)
    
    def forward(self, z):
        w = self.mapping_network(z)
        pos_emb = self.pos_emb().repeat(z.shape[0], 1, 1)
        h = pos_emb
        for block in self.blocks:
            h = block(h, w) #B,L,E
        
        y = self.norm(h, w)
        batch_num = y.shape[0] * y.shape[1]
        Efou = self.cord_emb().repeat(batch_num,1,1) # (BxL, PxP, E)
        rgb_s = self.f_theta(Efou, y) #(BxL, PxP, C) 
        rgb_s = rgb_s.view(-1, self.image_size//self.patch_size, self.image_size//self.patch_size, self.patch_size, self.patch_size, self.out_features)
        rgb_s = rgb_s.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, self.image_size, self.image_size, self.out_features) #B,H,W,C

        return rgb_s.permute(0,3,1,2) #B,C,H,W


# custom weights initialization called on netG and netD
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)


if __name__ == "__main__":
    latent_dim = 128
    torch.cuda.empty_cache()
    netG = Generator(emb_dim = 300,latent_dim = latent_dim, num_heads = 5, n_blocks = 3, mlp_ratio = 4,image_size=32, demodulation=False, out_features=3).cuda()
    netD = Discriminator(emb_dim = 300, num_heads = 5, n_blocks = 3, mlp_ratio = 4,image_size=32, in_channels=3).cuda()
    #netG.apply(weights_init)
    #netD.apply(weights_init)
    ########################################################################################################################
    acc = 0
    for param in netG.parameters():
        acc+=param.numel()
    print("Generator",acc)
    acc = 0
    for param in netD.parameters():
        acc+=param.numel()
    print("Discriminator",acc)
    ########################################################################################################################
    batch_size = 32
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0., 0., 0.), (1., 1., 1.))])
    trainset = torchvision.datasets.CIFAR10(root='./data', train = True, download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)
    iteration = (len(trainset)//batch_size)
    ########################################################################################################################
    g_lr = 0.002
    d_lr = 0.002
    beta1 = 0.0 
    beta2 = 0.99
    lam = 0.7
    lambda_real = 10.
    lambda_fake = 10.
    gamma = 0.999
    gen_optimizer = torch.optim.Adam(netG.parameters(), g_lr, (beta1, beta2))
    dis_optimizer = torch.optim.Adam(netD.parameters(),d_lr, (beta1, beta2))
    #gen_scheduler = torch.optim.lr_scheduler.ExponentialLR(gen_optimizer, gamma, last_epoch=- 1, verbose=False)
    ema = ExponentialMovingAverage(netG.parameters(), decay=0.995)
    fixed_z = torch.FloatTensor(np.random.normal(0, 1, (20, latent_dim))).cuda()

    nb_epoch = 200 
    display_freq = 500
    #criterion = nn.BCELoss().cuda()
    #############################################################################################################


    g_losses = []
    d_losses = []


    j = 0


    for epoch in range(nb_epoch):
        
        pbar = tqdm(enumerate(trainloader))
        for i, batch in pbar:
            im, labels = batch 
            im = im.cuda()
            im_augmented = DiffAugment(im, policy='color,translation,cutout', channels_first=True)
            cur_batch_size = im.shape[0]
            z = torch.randn(cur_batch_size, latent_dim).cuda()
            #######################################################################################
            zero_label= torch.zeros(cur_batch_size).cuda()# Size-> (cur_batch_size,1)
            one_label = torch.ones(cur_batch_size).cuda()# Size-> (cur_batch_size,1)
            
            
            fake_image=netG(z)
            augmented_fake = DiffAugment(fake_image, policy='color,translation,cutout', channels_first=True).detach()
        
            ###########################################################################################
            yhat_real = netD(im_augmented).flatten()
            real_loss = nn.BCEWithLogitsLoss()(yhat_real,one_label)
            bcr_real = F.mse_loss(yhat_real, netD(im).flatten())
            
            yhat_fake=netD(augmented_fake).flatten()
            fake_loss=nn.BCEWithLogitsLoss()(yhat_fake,zero_label)
            bcr_fake = F.mse_loss(yhat_fake, netD(fake_image).flatten())
            ############################################################################################
            ###
            ### Discriminator
            ###
            d_loss = ((real_loss + fake_loss) / 2) + bcr_real * lambda_real + bcr_fake * lambda_fake
            
            d_loss =(real_loss+fake_loss)/2 #+ lam*l2_reg#     YOUR CODE HERE
            #########################################################################################################################
            dis_optimizer.zero_grad()
            d_loss.backward(retain_graph = True) # we need to retain graph=True to be able to calculate the gradient in the g backprop
            #torch.nn.utils.clip_grad_norm_(netD.parameters(), 5.)
            dis_optimizer.step()

            
            ###
            ### Generator
            ###
        
            
            g_loss = nn.BCEWithLogitsLoss()(netD(fake_image).flatten(),one_label) #      YOUR CODE HERE
            gen_optimizer.zero_grad()
            g_loss.backward()
            #torch.nn.utils.clip_grad_norm_(netG.parameters(), 5.)
            gen_optimizer.step()
            ema.update()
            
            # Save Metrics
            
            d_losses.append(d_loss.item())
            g_losses.append(g_loss.item())
            
            avg_real_score = yhat_real.sigmoid().mean().item()
            avg_fake_score = yhat_fake.sigmoid().mean().item()

            pbar.set_description(f"it: {j}; g_loss: {g_loss}; d_loss: {d_loss}; avg_real_score: {avg_real_score}; avg_fake_score: {avg_fake_score}")
            
            if i % display_freq == 0:
                fake_im = netG(fixed_z).cpu().detach()
                
                un_norm = renorm(fake_im) # for visualization
                
                grid = torchvision.utils.make_grid(un_norm, nrow=5)
                pil_grid = to_pil(grid)  
                
                plt.imshow(pil_grid)
                plt.savefig('comparison.png')
                plt.show()
                
                
                plt.plot(range(len(g_losses)), g_losses, label='g loss')
                plt.plot(range(len(g_losses)), d_losses, label='d loss')
                
                plt.legend()
                plt.show()
                
            j += 1
            
            del z
            del yhat_real
            del yhat_fake
            del fake_image
            del zero_label
            del one_label
            del im
            del g_loss
            del d_loss
            del real_loss
            del fake_loss
        print(f"Iteration {epoch}")    
        #gen_scheduler.step()